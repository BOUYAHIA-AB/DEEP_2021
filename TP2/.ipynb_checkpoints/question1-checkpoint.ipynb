{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importer les packages nécessaire pour la réalisation de la question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from shutil import copyfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from question1 import separate_train_test\n",
    "import torch.optim as optim\n",
    "\n",
    "from poutyne import set_seeds, Model, ModelCheckpoint, CSVLogger, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalize(use_data = True) :\n",
    "    \n",
    "    \"\"\"calcul la moyenne et l'écart-type d'un jeu de données\n",
    "    \n",
    "    Params : \n",
    "    use data : vaut True si on va calculer la moyenne et l'écart-type à partir du dataset et Flase si on va\n",
    "                retourner la moyenne et l'écart-type de ImageNet\n",
    "    returne : moyenne et l'écart-type \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if use_data :\n",
    "        train_path = \"./data/train\"\n",
    "        train_data = ImageFolder(train_path, T.Compose([T.Resize([224,224]), T.ToTensor()]))\n",
    "\n",
    "        nb_samples = len(train_data)\n",
    "\n",
    "        loader = DataLoader(\n",
    "            train_data,\n",
    "            batch_size=nb_samples,\n",
    "            num_workers=0,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        mean = 0.\n",
    "        std = 0.\n",
    "        for data in loader:\n",
    "            batch_samples = data[0].shape[0]\n",
    "            data = data[0].view(batch_samples, data[0].shape[1], -1)\n",
    "            mean += data.mean(2).sum(0)\n",
    "            std += data.std(2).sum(0)\n",
    "            nb_samples += batch_samples\n",
    "\n",
    "        mean /= nb_samples\n",
    "        std /= nb_samples\n",
    "\n",
    "        normalize = T.Normalize(\n",
    "            mean=mean,\n",
    "            std=std)\n",
    "\n",
    "        return normalize\n",
    "\n",
    "    else :\n",
    "        normalize = T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "        return normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Séparer les données en test et train en utilsant la méthode separate_train_test() fournie avec l'énoncé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./data/images\"\n",
    "train_path = \"./data/train\"\n",
    "test_path = \"./data/test\"\n",
    "\n",
    "separate_train_test(dataset_path, train_path, test_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utiliser les valeurs du dataset d’entraînement pour normaliser les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_device = 0\n",
    "device = torch.device(\"cuda:%d\" % cuda_device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_classes = 200\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "n_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = get_normalize()\n",
    "\n",
    "train_data = ImageFolder(\n",
    "    train_path,\n",
    "    T.Compose([\n",
    "        T.Resize([224,224]),\n",
    "        T.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "test_data = ImageFolder(\n",
    "    test_path,\n",
    "    T.Compose([\n",
    "        T.Resize([224,224]),\n",
    "        T.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size, num_workers=0, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(save_path)\n",
    "    save_path = save_path \n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # Save the weights in a new file when the current model is better than all previous models.\n",
    "    best_checkpoint = ModelCheckpoint(os.path.join(save_path, 'best_epoch_{epoch}.ckpt'), monitor='val_acc', mode='max', \n",
    "                        save_best_only=True, restore_best=True, verbose=True)\n",
    "\n",
    "    # reduce learning rate if the val_acc don't improve for 3 epoch\n",
    "    scheduler = ReduceLROnPlateau(monitor='val_acc', mode='max', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "    callbacks = [best_checkpoint, scheduler]\n",
    "    \n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation aléatoire par défaut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18()\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(resnet18.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "callbacks = get_callbacks('default_initialisation')\n",
    "\n",
    "model = Model(resnet18, optimizer, loss_function, batch_metrics=['accuracy'],  device=device)\n",
    "\n",
    "model.fit_generator(train_loader, train_loader, epochs=n_epoch, callbacks=callbacks)\n",
    "\n",
    "test_loss, test_acc = model.evaluate_generator(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle pré-entraîné, mais en gelant tous les paramètres de convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_conv(resnet18):\n",
    "    for name, param in resnet18.named_parameters():\n",
    "        if (name.find('conv') != -1) or (name.find('bn') != -1):\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n",
    "freeze_conv(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(resnet18.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "model = Model(resnet18, optimizer, loss_function, batch_metrics=['accuracy'],  device=device)\n",
    "\n",
    "model.fit_generator(train_loader, train_loader, epochs=n_epoch, callbacks=callbacks)\n",
    "\n",
    "test_loss, (test_acc, test_f1) = model.evaluate_generator(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle pré-entraîné, mais en gelant uniquement les paramètres dans \"layer1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_layer1(resnet18):\n",
    "    for name, param in resnet18.named_parameters():\n",
    "        if name.startswith('layer1'):\n",
    "            param.requires_grad = False      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n",
    "freeze_layer1(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(resnet18.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "model = Model(resnet18, optimizer, loss_function, batch_metrics=['accuracy'],  device=device)\n",
    "\n",
    "model.fit_generator(train_loader, train_loader, epochs=n_epoch, callbacks=callbacks)\n",
    "\n",
    "test_loss, (test_acc, test_f1) = model.evaluate_generator(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modèle pré-entraîné, mais en laissant tous les paramètres se faire ajuster par backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(resnet18.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "model = Model(resnet18, optimizer, loss_function, batch_metrics=['accuracy'],  device=device)\n",
    "\n",
    "model.fit_generator(train_loader, train_loader, epochs=n_epoch, callbacks=callbacks)\n",
    "\n",
    "test_loss, (test_acc, test_f1) = model.evaluate_generator(test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
